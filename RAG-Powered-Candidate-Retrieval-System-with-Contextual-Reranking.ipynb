{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":2508632,"sourceType":"datasetVersion","datasetId":1519260}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"### **Objective**:\n*Build a RAG-powered system that retrieves and ranks the most relevant resumes for a job role using semantic search and reranking, enabling faster and smarter candidate screening.*\n\n","metadata":{}},{"cell_type":"code","source":"# Install required packages (run this cell if not already installed)\n!pip install -q torch transformers accelerate bitsandbytes langchain sentence-transformers faiss-cpu openpyxl datasets pypdf langchain-community langchain-huggingface ragatouille\n!pip install flashrank # rerank ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T09:28:57.941388Z","iopub.execute_input":"2025-07-10T09:28:57.942121Z","iopub.status.idle":"2025-07-10T09:30:51.064104Z","shell.execute_reply.started":"2025-07-10T09:28:57.942095Z","shell.execute_reply":"2025-07-10T09:30:51.063361Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.0/68.0 kB\u001b[0m \u001b[31m5.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m31.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m23.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m31.3/31.3 MB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m79.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.1/46.1 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.1/116.1 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m183.9/183.9 kB\u001b[0m \u001b[31m13.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m441.4/441.4 kB\u001b[0m \u001b[31m29.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m367.7/367.7 kB\u001b[0m \u001b[31m25.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.2/45.2 kB\u001b[0m \u001b[31m396.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.3/10.3 MB\u001b[0m \u001b[31m91.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m284.6/284.6 kB\u001b[0m \u001b[31m21.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.0/41.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m320.0/320.0 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m444.8/444.8 kB\u001b[0m \u001b[31m29.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m72.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.1/755.1 kB\u001b[0m \u001b[31m43.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.3/129.3 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nsigstore 3.6.1 requires rich~=13.0, but you have rich 14.0.0 which is incompatible.\npandas-gbq 0.26.1 requires google-api-core<3.0.0dev,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ngcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.12.0 which is incompatible.\nbigframes 1.36.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires pylibraft-cu12==24.12.*, but you have pylibraft-cu12 25.2.0 which is incompatible.\npylibcugraph-cu12 24.12.0 requires rmm-cu12==24.12.*, but you have rmm-cu12 25.2.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mCollecting flashrank\n  Downloading FlashRank-0.2.10-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: tokenizers in /usr/local/lib/python3.11/dist-packages (from flashrank) (0.21.0)\nCollecting onnxruntime (from flashrank)\n  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from flashrank) (1.26.4)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from flashrank) (2.32.3)\nRequirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from flashrank) (4.67.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->flashrank) (2.4.1)\nCollecting coloredlogs (from onnxruntime->flashrank)\n  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\nRequirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (25.2.10)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (24.2)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (3.20.3)\nRequirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime->flashrank) (1.13.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->flashrank) (2025.1.31)\nRequirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers->flashrank) (0.30.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (3.18.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (2024.12.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (6.0.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers->flashrank) (4.13.1)\nCollecting humanfriendly>=9.1 (from coloredlogs->onnxruntime->flashrank)\n  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->flashrank) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->flashrank) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->flashrank) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->flashrank) (2024.2.0)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime->flashrank) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->flashrank) (2024.2.0)\nDownloading FlashRank-0.2.10-py3-none-any.whl (14 kB)\nDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m97.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: humanfriendly, coloredlogs, onnxruntime, flashrank\nSuccessfully installed coloredlogs-15.0.1 flashrank-0.2.10 humanfriendly-10.0 onnxruntime-1.22.0\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import numpy as np\nimport os\nimport shutil\nimport pandas as pd","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:20:00.752535Z","iopub.execute_input":"2025-07-10T10:20:00.753125Z","iopub.status.idle":"2025-07-10T10:20:00.756836Z","shell.execute_reply.started":"2025-07-10T10:20:00.753100Z","shell.execute_reply":"2025-07-10T10:20:00.756157Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"from tqdm.notebook import tqdm\nfrom typing import Optional, List, Tuple\nfrom datasets import Dataset\nfrom langchain.document_loaders import DirectoryLoader, PyPDFLoader","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T09:33:56.386184Z","iopub.execute_input":"2025-07-10T09:33:56.387148Z","iopub.status.idle":"2025-07-10T09:33:58.484138Z","shell.execute_reply.started":"2025-07-10T09:33:56.387121Z","shell.execute_reply":"2025-07-10T09:33:58.483377Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"\n\ndef load_resumes(data_dir: str, csv_path: str = None) -> list:\n    \"\"\"Load resumes from directory and optional CSV metadata.\"\"\"\n    # Load CSV if provided (single line with error handling)\n    df = pd.read_csv(csv_path) if csv_path and os.path.exists(csv_path) else None\n    \n    if not os.path.exists(data_dir):\n        raise FileNotFoundError(f\"Directory not found: {data_dir}\")\n    \n    documents = []\n    for category in os.listdir(data_dir):\n        category_path = os.path.join(data_dir, category)\n        if not os.path.isdir(category_path):\n            continue\n            \n        # Load PDFs (removed progress bar for speed)\n        loader = DirectoryLoader(category_path, glob=\"*.pdf\", loader_cls=PyPDFLoader)\n        docs = loader.load()\n        \n        for doc in docs:\n            # Simplified metadata extraction\n            doc.metadata.update({\n                \"category\": category,\n                \"file_name\": os.path.basename(doc.metadata[\"source\"]),\n                \"id\": os.path.splitext(doc.metadata[\"source\"].split('/')[-1])[0]\n            })\n            \n            # Optional CSV merge (one-liner)\n            if df is not None:\n                if match := df[df[\"ID\"] == doc.metadata[\"id\"]].to_dict('records'):\n                    doc.metadata.update(match[0])\n        \n        documents.extend(docs)\n    print(documents[0])\n    return documents\n\n# Usage (2 lines)\nDATA_DIR = \"/kaggle/input/resume-dataset/data/data\"\ndocs = load_resumes(DATA_DIR, \"/kaggle/input/resume-dataset/Resume/Resume.csv\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T09:57:41.772231Z","iopub.execute_input":"2025-07-10T09:57:41.772835Z","iopub.status.idle":"2025-07-10T10:07:12.266205Z","shell.execute_reply.started":"2025-07-10T09:57:41.772811Z","shell.execute_reply":"2025-07-10T10:07:12.265513Z"}},"outputs":[{"name":"stdout","text":"page_content='PRE-PRESS GRAPHIC DESIGNER\nSummary\nCreative, hardworking designer seeking a full-time desktop job, educated as a graphic artist, past experience in business world as a desktop\npublisher laying out designs for printed mail and advertisements, in local government designing new websites with graphics for different agencies\nwithin the system, and later for the same government printing and reproduction center creating documents to be printed off a press or copiers.\nSkills\nAdobe InDesign, Photoshop, Illustrator, and Acrobat Professional\nStrongly familiar with Microsoft Word, Excel, PowerPoint, and Publisher / also QuarkXPress\nBasic knowledge of web development with Adobe Dreamweaver, HTML, WordPress\nAble to perform graphic design and administrative functions\nAble to work as a team player and independently\nExperienced using phone, fax, email, copiers and printers\nProvides excellent customer service (in-person, by phone, email, or interoffice mail)\nPrioritizes and calmly handles multiple projects and requests\nListens to directions, takes notes for later reference, follows procedures\nKnowledge of design setup on computer for jobs to be printed by outside vender or in-office copiers\nExperience\n01/2008\n \nto \nCurrent\nPre-Press Graphic Designer\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nCreate new designs for variety of items like manuals, newsletters, and posters.\nUse templates for updated documents like envelopes, letterheads, and business cards.\nProof jobs for initial and final customer approval.\nManufactures a high-quality PDF file digitally for proofing, photocopying and offset printing.\nPerforms file backup and organizes system for easy recovery.\nMaintains and monitors supply inventory and orders items when needed.\nOperates photocopying equipment, includes sending approved documents to printer.\nAssists in the bindery department, using the folder and manual paper cutter for small jobs.\nAlso can use bindery equipment, like the fastback and GBC binding of spines.\nMounts and laminates to foam boards, manually trims to size.\nEnsures timely submission of files to production.\n04/2000\n \nto \n01/2008\nWeb Designer\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nCreated new sites and made updates to current sites; created graphics to use on web pages; scanned documents and converted digital files\nfor links on sites; maintained updates and corrections on sites; answered email and phone call requests from departments about site changes;\nproofed pages with emails before sending live to internet.\n06/1998\n \nto \n02/2000\nDesktop Publisher\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nPerformed set-up and conversion of documents from Mac to PC then to UNIX systems; used QuarkXPress on Mac for the set-up of many\njobs; sent to network to be used by programmers for \"targeted\" direct mail printouts; trained new team staff members; helped with clean-up\nwhen company shut down.\n06/1997\n \nto \n03/2000\nGraphic Designer\n \nCompany Name\n \nï¼​ \nCity\n \n, \nState\nTemporary office jobs using Macintosh computers to design files to be printed for various companies like:.\nAlltel Publishing.\nCleveland School District.\nHKM Marketing Communications.\nNationwide Advertising.\nEducation and Training\nMay 1997\nBachelor of Fine Arts\n \nAlfred University\n \nï¼​ \nCity\n \n, \nState\nWork History\nCompany Name\nSkills\nadministrative functions, Acrobat, Adobe Dreamweaver, Photoshop, Advertising, backup, Basic, business cards, conversion, excellent customer\nservice, direct mail, email, fax, graphic design, graphics, HTML, Illustrator, Adobe InDesign, Mac, Macintosh computers, Marketing\nCommunications, Excel, mail, office, PowerPoint, Publisher, Microsoft Word, monitors, network, newsletters, takes notes, PDF, copiers, posters,\nprinter, printers, proofing, quality, QuarkXPress, supply inventory, team player, phone, UNIX, web development, web pages' metadata={'producer': 'Qt 4.8.7', 'creator': 'wkhtmltopdf 0.12.4', 'creationdate': '2021-08-08T15:32:08+05:30', 'title': '', 'source': '/kaggle/input/resume-dataset/data/data/DESIGNER/22506245.pdf', 'total_pages': 1, 'page': 0, 'page_label': '1', 'category': 'DESIGNER', 'file_name': '22506245.pdf', 'id': '22506245'}\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"# to create a raw k_b to feed into text spillter\nfrom langchain.docstore.document import Document as LangchainDocument\n\nRAW_KNOWLEDGE_BASE = [\n    LangchainDocument(\n        page_content=doc.page_content,\n        metadata={\n            \"Category\": doc.metadata[\"category\"],\n            \"filename\": doc.metadata[\"file_name\"],\n            \"id\" :doc.metadata[\"id\"]# If available\n        }\n    ) for doc in docs\n]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:07:23.565484Z","iopub.execute_input":"2025-07-10T10:07:23.565763Z","iopub.status.idle":"2025-07-10T10:07:23.598262Z","shell.execute_reply.started":"2025-07-10T10:07:23.565742Z","shell.execute_reply":"2025-07-10T10:07:23.597460Z"}},"outputs":[],"execution_count":27},{"cell_type":"code","source":"# splitting raw d_b to feed into embeddoing model so we can create vector db\nfrom langchain.text_splitter import RecursiveCharacterTextSplitter\n\n# We use a hierarchical list of separators specifically tailored for splitting Markdown documents\n# This list is taken from LangChain's MarkdownTextSplitter class\nMARKDOWN_SEPARATORS = [\n    \"\\n#{1,6} \",\n    \"```\\n\",\n    \"\\n\\\\*\\\\*\\\\*+\\n\",\n    \"\\n---+\\n\",\n    \"\\n___+\\n\",\n    \"\\n\\n\",\n    \"\\n\",\n    \" \",\n    \"\",\n]\n\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,  # The maximum number of characters in a chunk: we selected this value arbitrarily\n    chunk_overlap=100,  # The number of characters to overlap between chunks\n    add_start_index=True,  # If `True`, includes chunk's start index in metadata\n    strip_whitespace=True,  # If `True`, strips whitespace from the start and end of every document\n    separators=MARKDOWN_SEPARATORS,\n)\n\ndocs_processed = [] # this splitts the charectters into chuks and store\nfor doc in RAW_KNOWLEDGE_BASE:\n    docs_processed += text_splitter.split_documents([doc])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:07:36.754634Z","iopub.execute_input":"2025-07-10T10:07:36.754952Z","iopub.status.idle":"2025-07-10T10:07:37.510742Z","shell.execute_reply.started":"2025-07-10T10:07:36.754916Z","shell.execute_reply":"2025-07-10T10:07:37.509968Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"#choosng andloadng emeddng model\nfrom sentence_transformers import SentenceTransformer\n\n# To get the value of the max sequence_length, we will query the underlying `SentenceTransformer` object used in the RecursiveCharacterTextSplitter\nprint(f\"Model's maximum sequence length: {SentenceTransformer('thenlper/gte-small').max_seq_length}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:07:38.835619Z","iopub.execute_input":"2025-07-10T10:07:38.835899Z","iopub.status.idle":"2025-07-10T10:07:42.387241Z","shell.execute_reply.started":"2025-07-10T10:07:38.835877Z","shell.execute_reply":"2025-07-10T10:07:42.386520Z"}},"outputs":[{"name":"stdout","text":"Model's maximum sequence length: 512\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"from transformers import AutoTokenizer# model toknze the splited data\n\ntokenizer = AutoTokenizer.from_pretrained(\"thenlper/gte-small\") # here chuks get tokenized for vectors storage\nlengths = [len(tokenizer.encode(doc.page_content)) for doc in tqdm(docs_processed)]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:07:49.831435Z","iopub.execute_input":"2025-07-10T10:07:49.831711Z","iopub.status.idle":"2025-07-10T10:07:59.829543Z","shell.execute_reply.started":"2025-07-10T10:07:49.831690Z","shell.execute_reply":"2025-07-10T10:07:59.828798Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/18499 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aeb811e698ad44bb8692f13c2ea92c1e"}},"metadata":{}}],"execution_count":30},{"cell_type":"code","source":"EMBEDDING_MODEL_NAME = \"thenlper/gte-small\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:08:01.320379Z","iopub.execute_input":"2025-07-10T10:08:01.320818Z","iopub.status.idle":"2025-07-10T10:08:01.324872Z","shell.execute_reply.started":"2025-07-10T10:08:01.320792Z","shell.execute_reply":"2025-07-10T10:08:01.324219Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"from langchain.vectorstores import FAISS # stores vector databse\nfrom langchain_community.embeddings import HuggingFaceEmbeddings # fuction chain to perform embeddngs\nfrom langchain_community.vectorstores.utils import DistanceStrategy # for retriver to identify smlar docs","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:08:03.703117Z","iopub.execute_input":"2025-07-10T10:08:03.703849Z","iopub.status.idle":"2025-07-10T10:08:03.707348Z","shell.execute_reply.started":"2025-07-10T10:08:03.703824Z","shell.execute_reply":"2025-07-10T10:08:03.706650Z"}},"outputs":[],"execution_count":32},{"cell_type":"code","source":"embedding_model = HuggingFaceEmbeddings(\n    model_name=EMBEDDING_MODEL_NAME,\n    multi_process=True,\n    model_kwargs={\"device\": \"cuda\"},\n    encode_kwargs={\"normalize_embeddings\": True},  # Set `True` for cosine similarity\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:08:05.108200Z","iopub.execute_input":"2025-07-10T10:08:05.108760Z","iopub.status.idle":"2025-07-10T10:08:07.822145Z","shell.execute_reply.started":"2025-07-10T10:08:05.108739Z","shell.execute_reply":"2025-07-10T10:08:07.821462Z"}},"outputs":[],"execution_count":33},{"cell_type":"code","source":"KNOWLEDGE_VECTOR_DATABASE = FAISS.from_documents(\n    docs_processed, embedding_model, distance_strategy=DistanceStrategy.COSINE\n)\n# here we hve xomplete vector knoelgde base to perform query to retrive docs or perform llm to\n#generte ansrs\n\nKNOWLEDGE_VECTOR_DATABASE # high dimension matrx ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:08:09.756518Z","iopub.execute_input":"2025-07-10T10:08:09.757274Z","iopub.status.idle":"2025-07-10T10:09:00.597404Z","shell.execute_reply.started":"2025-07-10T10:08:09.757249Z","shell.execute_reply":"2025-07-10T10:09:00.596639Z"}},"outputs":[{"name":"stderr","text":"2025-07-10 10:08:14.878352: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752142094.900444     213 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752142094.907019     213 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/10 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d40ba89d61d47bb847012ae0cda7eae"}},"metadata":{}},{"execution_count":34,"output_type":"execute_result","data":{"text/plain":"<langchain_community.vectorstores.faiss.FAISS at 0x7bbee4e921d0>"},"metadata":{}}],"execution_count":34},{"cell_type":"code","source":"# its a doc retriver for vector db\nretriever = KNOWLEDGE_VECTOR_DATABASE.as_retriever(search_kwargs={\"k\": 30})\n\ndocs = retriever.invoke(\"\"\"\n  \"Machine Learning Engineer\" OR \"Data Scientist\" AND (\n    (\"Python\" AND (\"Scikit-learn\" OR \"TensorFlow\" OR \"PyTorch\"))  \n    (\"SQL\" AND (\"Spark\" OR \"Hadoop\" OR \"ETL\"))  \n    (\"AWS\" OR \"GCP\" OR \"Azure\" OR \"MLOps\")  \n    (\"Tableau\" OR \"Power BI\" OR \"data visualization\")  \n    (\"statistical analysis\" OR \"A/B testing\")  \n    (\"NLP\" OR \"LLM\" OR \"GenAI\" OR \"recommendation systems\")  \n  )  \n  NOT (\"intern\" OR \"student\")  \n  Years: \"3+ years\"  \n  Location: \"Remote\" OR \"Mumbai\" OR \"Bangalore\" OR \"Pune\"  \n\"\"\")\n\n# import pprint\n\n# pprint.pp(docs) # retrved 30 docs to lter perform rerank","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:09:04.859969Z","iopub.execute_input":"2025-07-10T10:09:04.860237Z","iopub.status.idle":"2025-07-10T10:09:13.485797Z","shell.execute_reply.started":"2025-07-10T10:09:04.860217Z","shell.execute_reply":"2025-07-10T10:09:13.485189Z"}},"outputs":[{"name":"stderr","text":"2025-07-10 10:09:09.680744: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752142149.702751     233 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752142149.709488     233 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a86fb9f131004b0380f0baede754a77c"}},"metadata":{}}],"execution_count":35},{"cell_type":"code","source":"# all corrected rearank code\nfrom langchain.retrievers import ContextualCompressionRetriever\nfrom langchain_community.document_compressors import FlashrankRerank\nfrom flashrank import Ranker, RerankRequest  # Explicitly import RerankRequest","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:09:13.486759Z","iopub.execute_input":"2025-07-10T10:09:13.487030Z","iopub.status.idle":"2025-07-10T10:09:13.490957Z","shell.execute_reply.started":"2025-07-10T10:09:13.487002Z","shell.execute_reply":"2025-07-10T10:09:13.490264Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"# Monkey patch the missing reference\nimport langchain_community.document_compressors.flashrank_rerank as flashrank_rerank","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:09:27.525123Z","iopub.execute_input":"2025-07-10T10:09:27.525392Z","iopub.status.idle":"2025-07-10T10:09:27.529003Z","shell.execute_reply.started":"2025-07-10T10:09:27.525372Z","shell.execute_reply":"2025-07-10T10:09:27.528371Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"flashrank_rerank.RerankRequest = RerankRequest\n\n# Initialize retriever\nretriever = KNOWLEDGE_VECTOR_DATABASE.as_retriever(search_kwargs={\"k\": 30})\n\n# Initialize compressor\ncompressor = FlashrankRerank(top_n=5)\n\n# Create compression retriever\ncompression_retriever = ContextualCompressionRetriever(\n    base_compressor=compressor,\n    base_retriever=retriever\n)\n\n# Run query\nquery =  \"\"\"\n  \"Machine Learning Engineer\" OR \"Data Scientist\" AND (\n    (\"Python\" AND (\"Scikit-learn\" OR \"TensorFlow\" OR \"PyTorch\"))  \n    (\"SQL\" AND (\"Spark\" OR \"Hadoop\" OR \"ETL\"))  \n    (\"AWS\" OR \"GCP\" OR \"Azure\" OR \"MLOps\")  \n    (\"Tableau\" OR \"Power BI\" OR \"data visualization\")  \n    (\"statistical analysis\" OR \"A/B testing\")  \n    (\"NLP\" OR \"LLM\" OR \"GenAI\" OR \"recommendation systems\")  \n  )  \n  NOT (\"intern\" OR \"student\")  \n  Years: \"3+ years\"  \n  Location: \"Remote\" OR \"Mumbai\" OR \"Bangalore\" OR \"Pune\"  \n\"\"\"\ncompressed_docs = compression_retriever.invoke(query)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:09:29.425812Z","iopub.execute_input":"2025-07-10T10:09:29.426339Z","iopub.status.idle":"2025-07-10T10:09:49.787954Z","shell.execute_reply.started":"2025-07-10T10:09:29.426316Z","shell.execute_reply":"2025-07-10T10:09:49.787111Z"}},"outputs":[{"name":"stderr","text":"2025-07-10 10:09:35.059557: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1752142175.081754     254 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1752142175.088370     254 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Chunks:   0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aff88a92e3c84e5b972b3c0439bb501d"}},"metadata":{}}],"execution_count":38},{"cell_type":"code","source":"# Print results\nprint([doc.metadata.get(\"id\") for doc in compressed_docs])\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:09:49.789175Z","iopub.execute_input":"2025-07-10T10:09:49.789477Z","iopub.status.idle":"2025-07-10T10:09:49.793818Z","shell.execute_reply.started":"2025-07-10T10:09:49.789447Z","shell.execute_reply":"2025-07-10T10:09:49.792959Z"}},"outputs":[{"name":"stdout","text":"['50328713', '22946204', '17823436', '83816738', '18448085']\n","output_type":"stream"}],"execution_count":39},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Print results with more details\nfor doc in compressed_docs:\n    print(doc)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:11:24.038880Z","iopub.execute_input":"2025-07-10T10:11:24.039457Z","iopub.status.idle":"2025-07-10T10:11:24.043646Z","shell.execute_reply.started":"2025-07-10T10:11:24.039435Z","shell.execute_reply":"2025-07-10T10:11:24.042965Z"}},"outputs":[{"name":"stdout","text":"page_content='ENGINEERING INTERN\nSkills\nC++, Python, MATLAB, Git, Bash, R, SQL (basic). Experienced in Linux/Unix and using high performance computing clusters.\nMachine Learning Tools and Libraries: Scikit-learn, Pandas, Seaborn, matplotlib, TensorFlow (basic). (I built a XGBoost\nmodel that has 77.5% accuracy in the Kaggle Titanic challenge.)\nComputational Fluid Dynamics and Discrete Element Method Codes\nCFD-DEM, OpenFOAM, CFD-ACE+Â®, FluentÂ®, COMSOLÂ®, LAMMPS, and LIGGGHTS.\nReservoir and Fracture Modeling Tools\nCMGÂ® for reservoir simulation; FracProÂ® for fracture simulation and analysis; Saphir for pressure transient analysis.\nExperimental and Statistical Methods\nSEM, AFM, Confocal Microscopy, Regression analysis, Statistical process control, Design of experiments.\nExperience\nENGINEERING INTERN\n \n08/2016\n \nï¼​ \n12/2016\n \nCompany Name\n \nState\nProject: Develop a cavings transport model for optimizing hole-cleaning operations.' metadata={'id': '50328713', 'relevance_score': 0.9871056, 'Category': 'ENGINEERING', 'filename': '50328713.pdf', 'start_index': 0}\npage_content='Highlights\nProg. Languages: \nC (5+ yrs), Python (3+ yrs), Java (3+ yrs), MATLAB (Simulink) (5+ yrs), R (2 yrs), Processing (2yrs), SQL(4+ yrs),\nPLC(2 yrs)\nDoc. Editing: \nWord/PPT/Excel, Pages/Numbers/Keynote, LATEX\nMechanical Design: \nAutoCAD (6 yrs), Solidworks (5+ yrs)\nMechanical Skills: \nMakerBot 3D print, Laser cut, Mill, Drill, Lathe Machine.\nStatistics Softwares: \nSTATA, SPSS\nDatabase Softwares: \nSQL Server (4 yrs), Navicat (2 yrs)\nOperating Systems: \nWindows 7/10, OS X\nExperience\nCompany Name\n \nJune 2016\n \nto \nCurrent\n \nR&D Product Development Engineer\n \nCity\nDesign and build a tail-sitter VTOL(vertical take off and landing) UAV(unmanned aerial vehicle) which.\ntakes off and lands vertically and travels horizontally.\nMain duties include but not limit to aerodynamics.\nmodeling, UAV control system design, mechanical manufacturing, simulation and tuning/experiments.\nCompany Name\n \nMay 2015\n \nto \nFebruary 2016\n \nResearch assistant\n \nCity' metadata={'id': '22946204', 'relevance_score': 0.95992935, 'Category': 'AUTOMOBILE', 'filename': '22946204.pdf', 'start_index': 0}\npage_content='Pilot Run and User Acceptance testing.\nApplication training, Go Live, Project sign-off.\nWork with end-users to define and execute test scenarios and ensure appropriate end user training.\nTechnical Responsibilities: Provide detailed system requirement to client(Hardware/Software).\nProduction Server Setup (Windows 2003 Server / RedHat Linux 2.5).\nDatabase setup (Oracle 10g R2.\nIBM DB2,MS SQL 2005).\nStandard database restore, Master Data preparation.\nApplication server installation and configuration(Jboss 5.1.0.GA & Tomcat6).\nSoftware deployments(ear,war etc.).\nMaintaining Versions and Deliverable.\nEducation\nBachelor of Engineering\n \n, \nInformation Technology\n \n6 2008\n \nUniversity of Mumbai\n \nIntegrated Trading and Manufacturing (ITM,An ERP by Base\nInformation) BI Tool \n: BI Base (Business Intelligence tool by Base Information) Information Technology\nPersonal Information\nComprehensive problem solving abilities, excellent verbal\nInterests\nPassport, Visa Details' metadata={'id': '17823436', 'relevance_score': 0.91915107, 'Category': 'BANKING', 'filename': '17823436.pdf', 'start_index': 819}\npage_content='INFORMATION TECHNOLOGY INTERN (TEST AUTOMATION ENGINEER)\nSummary\nOver 3 yearsÂ of experience serving as a key contributor across all software development life cycleÂ phases includingÂ analysis,\narchitectural design, prototyping, development, and testing of applicationÂ using Java/J2EE technologies in various domains.\nVery good understanding of Object Oriented Programming, Data Structure, Algorithms, Design Patterns and Distributed Systems.\nExcellent working experience in backendÂ development using different Spring modules like Spring Core ContainerÂ Module, AOP, MVC,\nSecurity, Data, Transaction Management etc.\nExperienced in developing Microservices with Spring Boot, Spring REST, Spring Cloud, etc.\nExtensive experience in developing Web interfaces using HTML5, CSS3, Bootstrap, SASS, LESS, JavaScript, jQuery, AngularJS,\nReactJS and BackboneJS.\nExperienced in working with SQL databases like MySQL, PostgreSQL, Oracle and have some knowledge of NoSQL databases like\nMongoDB.' metadata={'id': '83816738', 'relevance_score': 0.89844215, 'Category': 'INFORMATION-TECHNOLOGY', 'filename': '83816738.pdf', 'start_index': 0}\npage_content='DATA ANALYST\nProfessional Summary\nIndustrial and Systems Engineering graduate, certified Base SAS Programmer and a Lean Six Sigma Green Belt with strong background in\nstatistics, mathematics and logical problem solving looking for a dynamic opportunity in data driven fields of analytics and statistical modeling.\nCore Qualifications\nData Science Tools: R, Base SAS, Python (Numpy, Pandas, Matplotlib, Scikit- learn), SPSS, Minitab, MATLAB, Apache Spark, SQL, MS\nExcel, MS Visio, Tableau MySQL, Oracle Database, Microsoft Access Key Competencies: Data Extraction, Data Wrangling, Data Analysis,\nData Visualization, Regression Analysis (Linear, Logistic and Multinomial), Time Series Analysis, Association Rule Mining, Monte Carlo\nSimulation, Optimization, Random Forests\nExperience\n07/2016\n \nto \nCurrent\nData Analyst\n \nCompany Name\n \nï¼​ \nState\n09/2015\n \nto \n05/2016\nStudent Manager\n \nCompany Name\n \nï¼​ \nState' metadata={'id': '18448085', 'relevance_score': 0.8976784, 'Category': 'AUTOMOBILE', 'filename': '18448085.pdf', 'start_index': 0}\n","output_type":"stream"}],"execution_count":42},{"cell_type":"code","source":"\n\n# Define the source directory containing the PDFs\nDATA_DIR = \"/kaggle/input/resume-dataset/data/data\"\n\n# List of file IDs from your output (replace with actual IDs from compressed_docs)\nfile_ids = [doc.metadata.get(\"id\") for doc in compressed_docs]  # Example: ['id1', 'id2', 'id3', 'id4', 'id5']\n\n# Define the output directory to save the downloaded PDFs\noutput_dir = \"./downloaded_pdfs\"\nos.makedirs(output_dir, exist_ok=True)\n\n# Function to find and copy PDF files\ndef download_pdfs(file_ids, data_dir, output_dir):\n    for file_id in file_ids:\n        # Search for the PDF file in the data directory\n        for category in os.listdir(data_dir):\n            category_path = os.path.join(data_dir, category)\n            if not os.path.isdir(category_path):\n                continue\n            pdf_path = os.path.join(category_path, f\"{file_id}.pdf\")\n            if os.path.exists(pdf_path):\n                # Copy the PDF to the output directory\n                output_path = os.path.join(output_dir, f\"{file_id}.pdf\")\n                shutil.copy(pdf_path, output_path)\n                print(f\"Downloaded: {output_path}\")\n            else:\n                print(f\"File not found: {file_id}.pdf\")\n\n# Run the download function\ndownload_pdfs(file_ids, DATA_DIR, output_dir)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-10T10:14:50.264844Z","iopub.execute_input":"2025-07-10T10:14:50.265161Z","iopub.status.idle":"2025-07-10T10:14:50.466658Z","shell.execute_reply.started":"2025-07-10T10:14:50.265139Z","shell.execute_reply":"2025-07-10T10:14:50.465881Z"}},"outputs":[{"name":"stdout","text":"File not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nDownloaded: ./downloaded_pdfs/50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 50328713.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nDownloaded: ./downloaded_pdfs/22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 22946204.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 17823436.pdf\nDownloaded: ./downloaded_pdfs/17823436.pdf\nFile not found: 17823436.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nDownloaded: ./downloaded_pdfs/83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 83816738.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nDownloaded: ./downloaded_pdfs/18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\nFile not found: 18448085.pdf\n","output_type":"stream"}],"execution_count":43},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}